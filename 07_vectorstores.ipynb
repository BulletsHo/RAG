{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.cn/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** ä½¿ç”¨å‘é‡å­˜å‚¨å®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆ</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "æˆ‘ä»¬åœ¨å‰é¢çš„ notebook ä¸­äº†è§£å¹¶å°è¯•äº†åµŒå…¥æ¨¡å‹ã€‚è®¨è®ºäº†å®ƒåœ¨é•¿æ–‡æ¡£æ¯”è¾ƒä¸­çš„åº”ç”¨ï¼Œå¹¶ä»¥å®ƒä¸ºä¸»å¹²å®ç°äº†åŸºäºè¯­ä¹‰çš„æ¯”è¾ƒã€‚æœ¬ notebook å°†æŠŠè¿™ä¸ªæ€è·¯ç”¨åˆ°æ£€ç´¢æ¨¡å‹ä¸Šï¼Œæ¢ç´¢å¦‚ä½•é *å‘é‡å­˜å‚¨*æ¥æ„å»ºè‡ªåŠ¨ä¿å­˜å’Œæ£€ç´¢ä¿¡æ¯çš„èŠå¤©æœºå™¨äººç³»ç»Ÿã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "### **å­¦ä¹ ç›®æ ‡ï¼š**\n",
    "\n",
    "* ç†è§£è¯­ä¹‰ç›¸ä¼¼åº¦ç³»ç»Ÿæ˜¯æ€ä¹ˆæ–¹ä¾¿åœ°å®ç°æ£€ç´¢çš„ã€‚\n",
    "* å­¦ä¼šå°†æ£€ç´¢æ¨¡å—æ•´åˆåˆ°èŠå¤©æ¨¡å‹ç³»ç»Ÿä¸­ï¼Œä»¥åˆ›å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·¥ä½œæµï¼Œç”¨äºå®Œæˆæ–‡æ¡£æ£€ç´¢æˆ–å¯¹è¯å†…å­˜ç¼“å†²ç­‰ä»»åŠ¡ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **æ€è€ƒé—®é¢˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook ä¸ä¼šå°è¯•åŠ å…¥å±‚æ¬¡åŒ–æ¨ç†ï¼ˆhierachical reasoningï¼‰æˆ–éæœ´ç´ ï¼ˆnon-naiveï¼‰çš„ RAGï¼Œå¦‚è§„åˆ’æ™ºèƒ½ä½“ï¼ˆpalnning agentsï¼‰ã€‚æƒ³æƒ³éœ€è¦å¦‚ä½•è°ƒæ•´æ‰èƒ½è®©è¿™äº›ç»„ä»¶åœ¨ LCEL é“¾ä¸­è¿è¡Œã€‚\n",
    "* æ€è€ƒå°†å‘é‡å­˜å‚¨æ–¹æ¡ˆç”¨åœ¨è§„æ¨¡åŒ–éƒ¨ç½²çš„æœ€å¥½æ—¶æœºæ˜¯ä»€ä¹ˆï¼Œä»¥åŠä»€ä¹ˆæ—¶å€™éœ€è¦ç”¨ GPU è¿›è¡Œä¼˜åŒ–ã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "### **Notebook ç‰ˆæƒå£°æ˜ï¼š**\n",
    "\n",
    "* æœ¬ notebook æ˜¯ [**NVIDIA æ·±åº¦å­¦ä¹ åŸ¹è®­ä¸­å¿ƒ**](https://www.nvidia.cn/training/)çš„è¯¾ç¨‹[**ã€Šæ„å»ºå¤§è¯­è¨€æ¨¡å‹ RAG æ™ºèƒ½ä½“ã€‹**](https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/)ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œæœªç» NVIDIA æˆæƒä¸å¾—åˆ†å‘ã€‚\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç¯å¢ƒè®¾ç½®ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## ç¬¬ 1 éƒ¨åˆ†ï¼šRAG å·¥ä½œæµæ¦‚è¿°\n",
    "\n",
    "æ­¤ notebook å°†æ¢ç´¢å¤šä¸ªèŒƒå¼å¹¶ç»™å‡ºå‚è€ƒä»£ç ï¼Œä»¥å¸®åŠ©æ‚¨å¼€å§‹ä½¿ç”¨æœ€å¸¸è§ä¸€äº›çš„æ£€ç´¢å¢å¼ºå·¥ä½œæµã€‚å…·ä½“æ¥è¯´å°†æ¶µç›–ä»¥ä¸‹éƒ¨åˆ†ï¼ˆæ¯ä¸ªéƒ¨åˆ†å„æœ‰ä¾§é‡ï¼‰ï¼š\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***é€‚ç”¨äºäº¤äº’å¼å¯¹è¯çš„å‘é‡å­˜å‚¨å·¥ä½œæµï¼š***\n",
    "* ä¸ºæ–°å¯¹è¯ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†æ¶ˆæ¯æ­£æ–‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³æ¶ˆæ¯å¡«å……åˆ° LLM ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***å¤„ç†ä»»æ„æ–‡æ¡£çš„å·¥ä½œæµï¼š***\n",
    "* **å°†æ–‡æ¡£åˆ†å¿«å¹¶å¤„ç†æˆæœ‰ç”¨ä¿¡æ¯ã€‚**\n",
    "* ä¸ºæ¯ä¸ª**æ–°æ–‡æ¡£å—**ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†**å—æ­£æ–‡ï¼ˆchunk bodiesï¼‰**å­˜åˆ°å‘é‡å­˜å‚¨ä¸­ä»¥ä¾›æ£€ç´¢ã€‚\n",
    "* åœ¨å‘é‡å­˜å‚¨ä¸­æŸ¥è¯¢ç›¸å…³çš„**å—**ï¼Œç”¨æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **é€‚ç”¨äºä»»æ„æ–‡æ¡£ç›®å½•çš„æ‰©å±•å·¥ä½œæµï¼š**\n",
    "* å°†**æ¯ä¸ªæ–‡æ¡£**åˆ†ä¸ºå¤šä¸ªå—å¹¶å¤„ç†æˆæœ‰ç”¨çš„ä¿¡æ¯ã€‚\n",
    "* ä¸ºæ¯ä¸ªæ–°æ–‡æ¡£å—ç”Ÿæˆè¯­ä¹‰åµŒå…¥ã€‚\n",
    "* å°†å—æ­£æ–‡å­˜åˆ°**å¯æ‰©å±•çš„å‘é‡æ•°æ®åº“ä¸­ä»¥å®ç°å¿«é€Ÿæ£€ç´¢**ã€‚\n",
    "\t+ ***å¯é€‰ï¼š*åˆ©ç”¨æ›´å¤§ç³»ç»Ÿçš„å±‚æ¬¡åŒ–ç»“æ„æˆ–å…ƒæ•°æ®ç»“æ„ã€‚**\n",
    "* åœ¨**å‘é‡æ•°æ®åº“**ä¸­æŸ¥è¯¢ç›¸å…³çš„å—æ¥å¡«å…… LLM ä¸Šä¸‹æ–‡ã€‚\n",
    "\t+ *å¯é€‰ï¼š*ä¿®æ”¹/åˆæˆç»“æœä»¥è·å¾—æ›´å¥½çš„ LLM ç»“æœã€‚\n",
    "\n",
    "<br>  \n",
    "\n",
    "ä¸ RAG ç›¸å…³çš„ä¸€äº›é‡è¦æœ¯è¯­éƒ½å¯ä»¥åœ¨ [**LlamaIndex Concepts é¡µé¢**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html) æŸ¥åˆ°ï¼Œè¿™æ˜¯å­¦ä¹  LlamaIndex åŠ è½½å’Œæ£€ç´¢ç­–ç•¥çš„å¾ˆå¥½çš„èµ„æºã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨åœ¨å­¦ä¹ æ­¤ notebook çš„è¿‡ç¨‹ä¸­å‚è€ƒå®ƒï¼Œå¹¶é¼“åŠ±æ‚¨åœ¨è¯¾åè¯•è¯• LlamaIndex äº²æ‰‹ä½“ä¼šå®ƒçš„ä¼˜ç¼ºç‚¹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>  \n",
    "\n",
    "## **ç¬¬ 2 éƒ¨åˆ†ï¼š** ç”¨äºå¯¹è¯å†å²çš„ RAG\n",
    "\n",
    "åœ¨ä¹‹å‰çš„æ¢ç´¢ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†æ–‡æ¡£åµŒå…¥æ¨¡å‹çš„åŠŸèƒ½ï¼Œå¹¶ç”¨å®ƒæ¥åµŒå…¥ã€å­˜å‚¨å’Œæ¯”è¾ƒæ–‡æœ¬çš„è¯­ä¹‰å‘é‡è¡¨ç¤ºã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥åŠ¨æ‰‹å°†å…¶æ‰©å±•åˆ°å‘é‡å­˜å‚¨é¢†åŸŸï¼Œä½†å¦‚æœç”¨æ ‡å‡† API é…åˆæ¡†æ¶çš„è¯ï¼Œå°±èƒ½å‘ç°å®ƒå·²ç»æ›¿æˆ‘ä»¬å®Œæˆäº†å¾ˆå¤šç¹é‡çš„å·¥ä½œï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **ç¬¬ 1 æ­¥ï¼š** åˆ›å»ºä¸€æ®µå¯¹è¯\n",
    "\n",
    "æƒ³è±¡ä¸€æ®µ Llama-13B èŠå¤©æ™ºèƒ½ä½“å’Œä¸€åªåä¸º Beras çš„ç†Šä¹‹é—´çš„å¯¹è¯ã€‚è¿™æ®µå¯¹è¯åŒ…å«äº†å¤§é‡ç»†èŠ‚å’Œæ½œåœ¨çš„åˆ†æ”¯ï¼Œä¸ºæˆ‘ä»¬çš„ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "ä»ç„¶å¯ä»¥ç”¨ä¸Šä¸€ä¸ª notebook çš„æ‰‹åŠ¨åµŒå…¥ç­–ç•¥ï¼Œä½†æˆ‘ä»¬å®Œå…¨å¯ä»¥è®©å‘é‡æ•°æ®åº“æ›¿æˆ‘ä»¬åšï¼\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **ç¬¬ 2 æ­¥ï¼š** æ„å»ºå‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
    "\n",
    "ä¸ºäº†æµç¨‹åŒ–å¯¹è¯ä¸­çš„ç›¸ä¼¼æ€§æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡å­˜å‚¨æ¥å¸®åŠ©æˆ‘ä»¬è¿½è¸ªæ–‡æœ¬ï¼**å‘é‡å­˜å‚¨**ï¼ˆVector Storesï¼‰æˆ–è€…å«å‘é‡å­˜å‚¨ç³»ç»Ÿï¼Œå¯¹åµŒå…¥/æ¯”è¾ƒç­–ç•¥çš„å¤§éƒ¨åˆ†åº•å±‚ç»†èŠ‚åšäº†æŠ½è±¡ï¼Œä¸ºåŠ è½½å’Œæ¯”è¾ƒå‘é‡æä¾›äº†ä¸€ä¸ªç®€æ´çš„æ¥å£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ğŸ¦œï¸ğŸ”—](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "é™¤äº†å€ŸåŠ© API ç®€åŒ–æµç¨‹å¤–ï¼Œå‘é‡å­˜å‚¨è¿˜åœ¨èƒŒåå®ç°äº†è¿æ¥å™¨ï¼ˆconnectorï¼‰ã€é›†æˆï¼ˆintegrationï¼‰å’Œä¼˜åŒ–ã€‚æˆ‘ä»¬å°†ä» [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)å¼€å§‹ï¼Œå®ƒé›†æˆäº†å…¼å®¹ LangChain çš„åµŒå…¥æ¨¡å‹ [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss)ï¼Œä»è€Œå…è®¸åœ¨æœ¬åœ°å®ç°å¿«é€Ÿå¯æ‰©å±•çš„æµç¨‹ï¼\n",
    "\n",
    "\n",
    "**å…·ä½“æ¥è¯´ï¼š**\n",
    "\n",
    "1. æˆ‘ä»¬å¯ä»¥é€šè¿‡ `from_texts` æ„é€ å™¨å°†å¯¹è¯è¾“å…¥åˆ° [**FAISS å‘é‡å­˜å‚¨**](https://python.langchain.com/docs/integrations/vectorstores/faiss)ã€‚è¿™æ ·æˆ‘ä»¬çš„å¯¹è¯æ•°æ®å’ŒåµŒå…¥æ¨¡å‹å°±ä¼šç”¨æ¥åˆ›å»ºç´¢å¼•ã€‚\n",
    "2. ç„¶åï¼Œè¿™ä¸ªå‘é‡å­˜å‚¨å°±å¯ä»¥ä½œä¸ºæ£€ç´¢å™¨ï¼Œæ”¯æŒç”¨ LangChain è¿è¡Œæ—¶ API æ¥æ£€ç´¢æ–‡æ¡£ã€‚\n",
    "\n",
    "ä»¥ä¸‹å†…å®¹å±•ç¤ºäº†å¦‚ä½•æ„å»º FAISS å‘é‡å­˜å‚¨å¹¶ä½¿ç”¨ LangChain `vectorstore` API å°†å…¶ä½œä¸ºæ£€ç´¢å™¨ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.5 ms, sys: 11.4 ms, total: 71.9 ms\n",
      "Wall time: 722 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "ç°åœ¨ï¼Œæ£€ç´¢å™¨å¯ä»¥åƒä»»ä½•å…¶ä»–å¯è¿è¡Œçš„ LangChain ä¸€æ ·ç”¨äºæŸ¥è¯¢å‘é‡å­˜å‚¨ä¸­çš„æŸäº›ç›¸å…³æ–‡æ¡£ï¼š\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
       "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ£€ç´¢å·¥å…·ä»æˆ‘ä»¬çš„æŸ¥è¯¢ä¸­æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£ã€‚æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œä¸æ˜¯æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰ç”¨æˆ–æ¸…æ™°ã€‚æ¯”å¦‚ï¼Œå¦‚æœä¸æ˜¯å‡ºäºä¸Šä¸‹æ–‡ï¼Œæ£€ç´¢è¯¢é—®*â€œæ‚¨çš„å§“åâ€*æ—¶æŠŠ*â€œBerasâ€*æ£€ç´¢å‡ºæ¥å¯èƒ½ä¸æ˜¯ä¸ªå¥½äº‹ã€‚æå‰è€ƒè™‘åˆ°æ½œåœ¨çš„é—®é¢˜å¹¶è®© LLM ç»„ä»¶ç›¸äº’ååŒæ›´æœ‰å¯èƒ½è®© RAG è¾¾åˆ°å¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **ç¬¬ 3 æ­¥ï¼š** å°†å¯¹è¯æ£€ç´¢åŠŸèƒ½æ•´åˆåˆ°æˆ‘ä»¬çš„é“¾ä¸­\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å·²æŠŠæ£€ç´¢å™¨ç»„ä»¶ä½œä¸ºä¸€ä¸ªé“¾äº†ï¼Œå¯ä»¥åƒä»¥å‰ä¸€æ ·å°†å…¶æ•´åˆåˆ°ç°æœ‰çš„èŠå¤©ç³»ç»Ÿä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æ„å»ºä¸€ä¸ª***ä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„ RAG*** äº†ï¼Œå…¶ä¸­ï¼š\n",
    "* **é»˜è®¤æƒ…å†µä¸‹ï¼Œæ£€ç´¢å™¨å§‹ç»ˆåœ¨æ£€ç´¢ä¸Šä¸‹æ–‡ã€‚**\n",
    "* **ç”Ÿæˆå™¨æ ¹æ®æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ‰§è¡Œæ“ä½œã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">It seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Rocky Mountains, which Beras mentioned was one of the reasons for his curiosity about the mountain range.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mIt seems that Beras lives in the Arctic. The cooler climate there is quite different from the warm climate found in\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Rocky Mountains, which Beras mentioned was one of the reasons for his curiosity about the mountain range.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "å¤šè¯•å‡ ä¸ªè°ƒç”¨ï¼Œçœ‹çœ‹æ–°é…ç½®çš„æ•ˆæœã€‚æ— è®ºæ‚¨é€‰æ‹©çš„æ˜¯å“ªä¸ªæ¨¡å‹ï¼Œéƒ½å¯ä»¥å…ˆä»ä¸‹é¢çš„å‡ ä¸ªé—®é¢˜å¼€å§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. However, your </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question asks about their specific location. The Rocky Mountains, or simply the Rockies, are located in the western</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">part of North America. They stretch all the way from British Columbia and Alberta in Canada, and go through the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">states of Montana, Idaho, Wyoming, Colorado, and New Mexico in the United States. It's a truly vast and remarkable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">natural formation!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. However, your \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion asks about their specific location. The Rocky Mountains, or simply the Rockies, are located in the western\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpart of North America. They stretch all the way from British Columbia and Alberta in Canada, and go through the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstates of Montana, Idaho, Wyoming, Colorado, and New Mexico in the United States. It's a truly vast and remarkable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnatural formation!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Hello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question about their location, they do not directly border California. In fact, they run north to south, starting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">from western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">into New Mexico. So, while they're not close to California, they certainly are a significant part of the North </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">American landscape. Is there anything else you'd like to know about them?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mHello there! The Rocky Mountains are a stunning range of mountains that span across North America. To answer your \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion about their location, they do not directly border California. In fact, they run north to south, starting \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfrom western Canada and stretching all the way down through the states of Montana, Idaho, Wyoming, Colorado, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minto New Mexico. So, while they're not close to California, they certainly are a significant part of the North \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAmerican landscape. Is there anything else you'd like to know about them?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Unfortunately, the context doesn't provide information on Beras's exact location or the distance from the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. However, since Beras mentioned living in the arctic, it's safe to say that he's quite far from the Rocky</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. The Rocky Mountains are located in North America, mainly in the western United States and Canada.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mUnfortunately, the context doesn't provide information on Beras's exact location or the distance from the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. However, since Beras mentioned living in the arctic, it's safe to say that he's quite far from the Rocky\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. The Rocky Mountains are located in North America, mainly in the western United States and Canada.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>  \n",
    "\n",
    "æ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°æŠŠè¿™ä¸ªä¿æŒåœ¨çº¿ï¼ˆalways-onï¼‰çš„æ£€ç´¢èŠ‚ç‚¹æ”¾åˆ°å¾ªç¯é‡Œæ•ˆæœå¾ˆä¸é”™ï¼Œå› ä¸ºç›®å‰è¾“å…¥ LLM çš„ä¸Šä¸‹æ–‡ä»ç„¶ç›¸å¯¹è¾ƒå°ã€‚æœ‰å¿…è¦åå¤å°è¯•åµŒå…¥å¤§å°ã€ä¸Šä¸‹æ–‡é™åˆ¶ç­‰é…ç½®ï¼Œæ¥æ›´å¥½åœ°é¢„æµ‹æ¨¡å‹è¡¨ç°ï¼Œå¹¶è¡¡é‡ä¸ºæé«˜æ€§èƒ½å€¼å¾—åšå‡ºä½•ç§åŠªåŠ›ã€‚\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **ç¬¬ 4 æ­¥ï¼š** è‡ªåŠ¨å¯¹è¯å­˜å‚¨\n",
    "\n",
    "ç°åœ¨å‘é‡å­˜å‚¨å·²ç»å¯ä»¥å·¥ä½œäº†ï¼Œæˆ‘ä»¬æœ€åå†åšä¸€ä¸ªé›†æˆï¼šåŠ ä¸€ä¸ªè°ƒç”¨ `add_texts` æ›´æ–°å­˜å‚¨çŠ¶æ€çš„è¿è¡Œæ—¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">remember that it's a vast and diverse landscape primarily known for its breathtaking natural beauty rather than its</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ice cream offerings. However, Beras, I assure you that exploring the Rocky Mountains would be an unforgettable </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">experience filled with awe-inspiring vistas, abundant wildlife, and countless opportunities for adventure. Now, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">where were we in our conversation about this incredible mountain range?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWhile I'm sure the Rocky Mountains would provide a magnificent backdrop for enjoying ice cream, it's essential to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mremember that it's a vast and diverse landscape primarily known for its breathtaking natural beauty rather than its\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mice cream offerings. However, Beras, I assure you that exploring the Rocky Mountains would be an unforgettable \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexperience filled with awe-inspiring vistas, abundant wildlife, and countless opportunities for adventure. Now, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhere were we in our conversation about this incredible mountain range?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Well, Beras, based on our conversation, it seems your favorite food might be ice cream! You did mention enjoying </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ice cream in the Rocky Mountains. It's always a delight to enjoy a good scoop, isn't it?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWell, Beras, based on our conversation, it seems your favorite food might be ice cream! You did mention enjoying \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mice cream in the Rocky Mountains. It's always a delight to enjoy a good scoop, isn't it?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">I see, Beras! I must have misunderstood your statement about enjoying ice cream in the Rocky Mountains. While it's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">clear you love a good scoop in a beautiful setting, I apologize for the oversight. It's fascinating to know that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">honey is your favorite. It's such a versatile and naturally sweet food, perfect for a big blue bear like you!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mI see, Beras! I must have misunderstood your statement about enjoying ice cream in the Rocky Mountains. While it's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mclear you love a good scoop in a beautiful setting, I apologize for the oversight. It's fascinating to know that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhoney is your favorite. It's such a versatile and naturally sweet food, perfect for a big blue bear like you!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Of course, Beras! I do now. While I had initially thought ice cream might be your favorite food due to your mention</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of enjoying it in the Rocky Mountains, I stand corrected. Your favorite food is honey. I apologize for any </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">confusion earlier. It's quite fascinating because honey is indeed a versatile and naturally sweet food. I </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">appreciate your sharing this with me.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mOf course, Beras! I do now. While I had initially thought ice cream might be your favorite food due to your mention\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof enjoying it in the Rocky Mountains, I stand corrected. Your favorite food is honey. I apologize for any \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconfusion earlier. It's quite fascinating because honey is indeed a versatile and naturally sweet food. I \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mappreciate your sharing this with me.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "ä¸åŒäºå°†ä¸Šä¸‹æ–‡æ³¨å…¥ LLM çš„æ›´è‡ªåŠ¨åŒ–çš„å…¨æ–‡æœ¬ï¼ˆfull-textï¼‰æˆ–åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼Œè¿™æ ·å¯é¿å…ä¸Šä¸‹æ–‡é•¿åº¦å¤±æ§ã€‚è¿™ç§ç­–ç•¥è™½ç„¶ç§°ä¸ä¸Šå®Œå…¨å¯é ï¼Œä½†å¯¹äºéç»“æ„åŒ–çš„å¯¹è¯æ¥è¯´å·²ç»æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ”¹è¿›äº†ï¼ˆç”šè‡³ä¸éœ€è¦å€ŸåŠ©ä¸€ä¸ªå¼ºå¤§çš„æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åšæ§½ä½å¡«å……ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 3 éƒ¨åˆ† [ç»ƒä¹ ]ï¼š** ç”¨ RAG è¿›è¡Œæ–‡æ¡£å—æ£€ç´¢\n",
    "\n",
    "é‰´äºæˆ‘ä»¬ä¹‹å‰å¯¹æ–‡æ¡£åŠ è½½çš„æ¢ç´¢ï¼Œæ‚¨åº”è¯¥å·²ç»ç†Ÿæ‚‰å¯¹æ•°æ®å—åµŒå…¥å’Œæ£€ç´¢äº†ã€‚ç°åœ¨å€¼å¾—èŠ±ç‚¹æ—¶é—´ç»§ç»­è¿‡ä¸€éï¼Œå› ä¸ºæŠŠ RAG ç”¨åœ¨æ–‡æ¡£ä¸Šæ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼šå®ƒçœ‹èµ·æ¥ä¼¼ä¹å¼€ç®±å³ç”¨ï¼Œä½†æƒ³è®©å®ƒåœ¨å®é™…åº”ç”¨ä¸­ä¿æŒå¯é çš„æ€§èƒ½éœ€è¦éå¸¸è°¨æ…åœ°ä¼˜åŒ–ã€‚æˆ‘ä»¬ä¹Ÿå€Ÿæ­¤æœºä¼šå›é¡¾ä¸€ä¸‹åŸºæœ¬çš„ LCEL æŠ€èƒ½ï¼\n",
    "\n",
    "<br> \n",
    "\n",
    "### **ç»ƒä¹ ï¼š**\n",
    "\n",
    "æ‚¨å¯èƒ½è¿˜è®°å¾—ä¹‹å‰æˆ‘ä»¬ç”¨ [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) åŠ è½½äº†ä¸€äº›æ¯”è¾ƒçŸ­çš„æ–‡ç« ï¼š\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "æ ¹æ®æ‰€å­¦ï¼Œé€‰æ‹©å‡ ä¸ªè®ºæ–‡ï¼Œå¹¶å¼€å‘ä¸€ä¸ªèƒ½è®¨è®ºè¿™äº›è®ºæ–‡çš„èŠå¤©æœºå™¨äººï¼\n",
    "\n",
    "<br>  \n",
    "\n",
    "è™½ç„¶è¿™æ˜¯ä¸€é¡¹ç›¸å½“è‰°å·¨çš„ä»»åŠ¡ï¼Œä½†ä¸‹é¢å°†æä¾›**å¤§éƒ¨åˆ†**å®ç°è¿‡ç¨‹ã€‚æ¼”ç¤ºè¿‡åï¼Œè®¸å¤šå¿…é¡»çš„ç¯èŠ‚å°±å·²ç»å®ç°å¥½äº†ï¼Œæ‚¨çœŸæ­£çš„ä»»åŠ¡æ˜¯å°†å®ƒä»¬é›†æˆåˆ°æœ€ç»ˆçš„ `retrieval_chain`ã€‚æ‚¨ä¼šåœ¨æœ€åä¸€ä¸ª notebook æŠŠå®ƒä»¬é›†æˆåˆ°é“¾ä¸­æ¥å®Œæˆè¯„ä¼°æµ‹è¯•ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 1ï¼š** è½½å…¥å¹¶åˆ†å—æ‚¨çš„æ–‡æ¡£\n",
    "\n",
    "ä»¥ä¸‹ä»£ç æä¾›äº†ä¸€äº›å¯ä»¥è½½å…¥åˆ° RAG é“¾çš„é»˜è®¤è®ºæ–‡ã€‚æ‚¨å¯ä»¥æ ¹æ®éœ€è¦é€‰æ›´å¤šçš„è®ºæ–‡ï¼Œä½†è¦æ³¨æ„é•¿æ–‡æ¡£çš„å¤„ç†æ—¶é—´ä¹Ÿæ›´é•¿ã€‚å…¶ä¸­è¿˜æœ‰ä¸€äº›åˆ©äºæé«˜ RAG æ€§èƒ½çš„ç®€åŒ–å‡è®¾åŠå¤„ç†æ­¥éª¤ï¼š\n",
    "\n",
    "* æ–‡æ¡£ä»…æˆªå–â€œå‚è€ƒâ€œâ€ï¼ˆReferencesï¼‰éƒ¨åˆ†ä¹‹å‰çš„å†…å®¹ã€‚é˜²æ­¢ç³»ç»Ÿè€ƒè™‘å†—é•¿å’Œä¸é‡è¦çš„å¼•ç”¨å’Œé™„å½•ã€‚\n",
    "* æœ‰ä¸€ä¸ªèƒ½æä¾›å…¨å±€è§†è§’çš„åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡æ¡£çš„æ•°æ®å—ã€‚å¦‚æœæ‚¨çš„å·¥ä½œæµå¹¶ä¸æ˜¯æ¯æ¬¡æ£€ç´¢éƒ½æä¾›å…ƒæ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°æ®å—å°±ä¼šå¾ˆæœ‰ç”¨ï¼Œç”šè‡³å¯ä»¥åœ¨åˆé€‚çš„æ—¶å€™ä½œä¸ºæ›´é«˜ä¼˜å…ˆçº§ä¿¡æ¯çš„ä¸€éƒ¨åˆ†ã€‚\n",
    "* æ­¤å¤–ï¼Œè¿˜ä¼šæ’å…¥å…ƒæ•°æ®æ¡ç›®ä»¥æä¾›å¸¸è§„ä¿¡æ¯ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä¼šæœ‰ä¸€äº›èåˆè¿›äº†å…ƒæ•°æ®çš„è·¨æ–‡æ¡£æ•°æ®å—ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼š** ***ä¸ºæ‰§è¡Œè¯„ä¼°ï¼Œè¯·è‡³å°‘æ”¾è¿›ä¸€ç¯‡å‘è¡¨æ—¶é—´ä¸è¶…è¿‡ä¸€ä¸ªæœˆçš„è®ºæ–‡ï¼***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
       "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
       "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
       "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
       "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
       "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
       "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
       "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
       "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
       "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
       "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 45\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
       "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
       "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
       "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
       "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
       "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
       "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
       "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
       "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
       "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 46\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
       "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
       "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
       "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
       "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
       "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
       "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
       "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
       "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
       "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
       "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
       "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
       "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
       "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 40\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
       "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
       "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
       "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
       "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
       "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
       "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
       "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
       "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
       "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 21\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
       "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
       "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
       "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
       "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
       "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
       "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 44\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
       "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
       "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
       "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
       "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
       "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
       "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
       "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
       "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
       "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
       "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
       "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
       "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")\n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "### **ä»»åŠ¡ 2ï¼š** æ„å»ºæ–‡æ¡£å‘é‡å­˜å‚¨\n",
    "\n",
    "æˆ‘ä»¬ç°åœ¨å·²ç»æœ‰äº†æ‰€æœ‰ç»„ä»¶ï¼Œå¯ä»¥ç»§ç»­å›´ç»•å®ƒä»¬åˆ›å»ºç´¢å¼•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 640 ms, sys: 64.9 ms, total: 705 ms\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "æ¥ç€åƒä¸‹é¢è¿™æ ·æŠŠç´¢å¼•åˆå¹¶ä¸ºä¸€ä¸ªï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 238 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **ä»»åŠ¡ 3ï¼š[ç»ƒä¹ ]** å®ç° RAG é“¾\n",
    "\n",
    "ç»ˆäºï¼Œä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæ¥å®ç° RAG å·¥ä½œæµå§ï¼å›é¡¾ä¸€ä¸‹ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ï¼š\n",
    "\n",
    "* ä¸€ç§ç”¨å‘é‡å­˜å‚¨ä»é›¶åˆ›å»ºå¯¹è¯è®°å¿†çš„æ–¹æ³•ï¼ˆç”¨ `default_FAISS()` åˆå§‹åŒ–ï¼‰\n",
    "* é€šè¿‡ `ArxivLoader` é¢„åŠ è½½äº†åŒ…æ‹¬æ–‡æ¡£ä¿¡æ¯çš„å‘é‡å­˜å‚¨ï¼ˆå­˜åœ¨ `docstore` é‡Œï¼‰ã€‚\n",
    "\n",
    "å†å€ŸåŠ©å‡ ä¸ªå·¥å…·ï¼Œå°±èƒ½é›†æˆæ‚¨çš„é“¾äº†ï¼æˆ‘ä»¬è¿˜æä¾›äº†å‡ ä¸ªé¢å¤–çš„ä¾¿æ·å·¥å…·ï¼ˆ`doc2str` åŠ `RPrint`ï¼‰ï¼Œæ‚¨å¯ä»¥é…Œæƒ…ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä¸€äº›å¯åŠ¨æç¤ºè¯å’Œç»“æ„å·²ç»å®šä¹‰å¥½äº†ã€‚\n",
    "\n",
    "> **åŸºäºä¸Šè¿°è¿™äº›ï¼š** å®ç° `retrieval_chain` å§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n (Answer only from retrieval. Only cite </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sources that are used. Make your response conversational.)'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\nNone\\n\\n Document Retrieval:\\nNone\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite \u001b[0m\n",
       "\u001b[32msources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG is a status system that is often used to indicate the status or health of a project. It stands for Red, Amber, and Green, with each color representing a different status.\n",
      "\n",
      "* Red typically represents a problem or issue that needs immediate attention. It could indicate that the project is behind schedule, over budget, or not meeting its objectives.\n",
      "* Amber represents a warning or caution that the project may be facing issues in the future. It could indicate that the project is at risk of falling behind schedule or going over budget, or that there are potential roadblocks that may need to be addressed.\n",
      "* Green represents a positive status, indicating that the project is on track and meeting its objectives.\n",
      "\n",
      "The RAG status system is a simple and visual way to communicate the status of a project to stakeholders, making it easy to understand and interpret. It is often used in project management, but can also be used in other contexts where tracking the status of a project or process is important."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : lambda d: None})\n",
    "    | RunnableAssign({'context' : lambda d: None})\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **ä»»åŠ¡ 4ï¼š** ä¸ Gradio èŠå¤©æœºå™¨äººäº¤äº’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [],
   "source": [
    "# chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "# try:\n",
    "#     demo.launch(debug=True, share=True, show_api=False)\n",
    "#     demo.close()\n",
    "# except Exception as e:\n",
    "#     demo.close()\n",
    "#     print(e)\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **ç¬¬ 4 éƒ¨åˆ†ï¼š** ä¿å­˜ç´¢å¼•ä»¥ç”¨äºè¯„ä¼°\n",
    "\n",
    "å®ç° RAG é“¾åï¼Œè¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading)ä¿å­˜æ‚¨ç§¯ç´¯å‡ºæ¥çš„å‘é‡å­˜å‚¨ã€‚æœ€åçš„è¯„ä¼°ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    }
   ],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "å¦‚æœæ‰€æœ‰å†…å®¹éƒ½å·²æ­£ç¡®ä¿å­˜ï¼Œå°±å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç ä» `tgz` å‹ç¼©æ–‡ä»¶æ‹¿åˆ°ç´¢å¼•äº†ï¼ˆåªè¦å®‰è£…å¥½äº† pip ç¯å¢ƒï¼‰ã€‚å½“æ‚¨ç¡®è®¤è¿™ä¸ªä»£ç å•å…ƒèƒ½æ‹¿åˆ°æ‚¨çš„ç´¢å¼•ä¹‹åï¼ŒæŠŠ `docstore_index.tgz` ä¸‹è½½ä¸‹æ¥ï¼Œä¸‹ä¸ª notebook ä¼šç”¨åˆ°ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n",
      ". To demonstrate, we build an index using the DrQA [5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality\\nSpeci\\ufb01city\\nBART better\\n7.1%\\n16.8%\\nRAG better\\n42.7%\\n37.4%\\nBoth good\\n11.7%\\n11.8%\\nBoth poor\\n17.7%\\n6.9%\\nNo majority\\n20.8%\\n20.1%\\nTable 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO\\nJeopardy QGen\\nGold\\n89.6%\\n90.0%\\nBART\\n70.7%\\n32.4%\\nRAG-Token\\n77.8%\\n46.8%\\nRAG-Seq.\\n83.5%\\n53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classi\\ufb01cation task, both RAG models are equivalent.\\nModel\\nNQ\\nTQA\\nWQ\\nCT\\nJeopardy-QGen\\nMSMarco\\nFVR-3\\nFVR-2\\nExact Match\\nB-1\\nQB-1\\nR-L\\nB-1\\nLabel Accuracy\\nRAG-Token-BM25\\n29.7\\n41.5\\n32.1\\n33.1\\n17.5\\n22.3\\n55.5\\n48.4\\n75.1\\n91.6\\nRAG-Sequence-BM25\\n31.8\\n44.1\\n36.6\\n33\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "----\n",
    "\n",
    "## **ç¬¬ 5 éƒ¨åˆ†ï¼š** æ€»ç»“\n",
    "\n",
    "æ­å–œï¼å¦‚æœæ‚¨çš„ RAG é“¾èƒ½æ­£å¸¸è¿è¡Œï¼Œå°±ç»§ç»­è¿›å…¥ 08_evaluation.ipynb è¿›è¡Œ **RAG è¯„ä¼°**å§ï¼\n",
    "\n",
    "### <font color=\"#76b900\">**éå¸¸å¥½ï¼**</font>\n",
    "\n",
    "### **æ¥ä¸‹æ¥**ï¼š\n",
    "**[å¯é€‰]** å›é¡¾ notebook é¡¶éƒ¨çš„â€œæ€è€ƒé—®é¢˜â€ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
